{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import pandas as pd\n",
    "import shapely\n",
    "import shapely.geometry\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from maskrcnn.preprocess.sample_aoi import load_df, aoi_to_chip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR_SHP = 'data/Shapefile/SHP2010/mglu2010v5_0/poligonos_urbanos.shp'\n",
    "IN_DIR_CEN = 'data/CPV/Raw/ITER2010/ITER_NALDBF10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_LOC_DIR = 'data/Experiment3/census.shp'\n",
    "OUT_IMG_DIR = 'data/Experiment3/aoi.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read urban shapefiles\n",
    "df_shp = gpd.read_file(IN_DIR_SHP)\n",
    "df_shp = df_shp.to_crs({'init': 'epsg:4326'})\n",
    "for col in df_shp.columns:\n",
    "    if col.startswith('CVE'):\n",
    "        df_shp[col] = df_shp[col].astype(int)\n",
    "df_shp.columns = ['ent', 'mun', 'loc', 'NOM_LOC', 'geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read census data\n",
    "df_cen = load_df(IN_DIR_CEN, drop=False)\n",
    "df_cen = gpd.GeoDataFrame(\n",
    "    df_cen,\n",
    "    geometry=[shapely.geometry.Point(x, y) for x, y in\n",
    "              zip(df_cen['lon'].values, df_cen['lat'].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    # when using inner join mode, two localities are dropped\n",
    "    # from df_shp\n",
    "    # because one of them has VPH variables as N/D and\n",
    "    # one has those as * (masked)\n",
    "    # dropped seems fine since we are going to be using those\n",
    "    # data for validation\n",
    "    df_shp, df_cen, how='inner', on=['ent', 'mun', 'loc'],\n",
    "    # most points recorded in the census is in the polygon\n",
    "    # with some exceptions\n",
    "    # all the exceptions are cases where points lie close to\n",
    "    # the census tract but are outside it, b/c the census tract\n",
    "    # is non convex or just b/c measurement errors\n",
    "    # I'm sure that they are the same census block though\n",
    "    # they are never too far apart from each other\n",
    "    suffixes=('', '_point'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['geometry_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the smaller half of the localities\n",
    "# some are so big that this exercise does not make sense\n",
    "area_median = df['geometry'].area.median()\n",
    "df = df.loc[df['geometry'].area < area_median, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vph_cols = [col for col in df.columns if col.startswith('VPH')]\n",
    "for col in ['VPH_PISOTI', 'VPH_1DOR', 'VPH_1CUART', 'VPH_2CUART',\n",
    "            'VPH_S_ELEC', 'VPH_AGUAFV', 'VPH_NODREN', 'VPH_SNBIEN']:\n",
    "    vph_cols.remove(col)\n",
    "\n",
    "# compute asset score via pca\n",
    "centered = (df.loc[:, vph_cols].values -\n",
    "            df.loc[:, vph_cols].values.mean(axis=0)[np.newaxis, :])\n",
    "m = PCA(n_components=1)\n",
    "df = pd.concat([\n",
    "    df.reset_index(drop=True),\n",
    "    pd.DataFrame(m.fit_transform(centered),\n",
    "                 columns=['cen_asset'])],\n",
    "    axis=1)\n",
    "\n",
    "# # compute asset score via summing\n",
    "# df.loc[:, 'cen_asset_score_sum'] = df.loc[:, vph_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct pairs\n",
    "centroids = np.array([df['geometry'].centroid.x.values,\n",
    "                      df['geometry'].centroid.y.values]).T\n",
    "tree = scipy.spatial.cKDTree(centroids)\n",
    "# find neighbors\n",
    "k = 4\n",
    "tree_d, tree_i = tree.query(centroids, k=k)\n",
    "tree_d = tree_d[:, 1:]\n",
    "tree_i = tree_i[:, 1:]\n",
    "_, tree_j = np.meshgrid(range(k - 1), range(centroids.shape[0]))\n",
    "pairs = np.array([\n",
    "    # max distance between centroids: sqrt(area_median)\n",
    "    tree_j[tree_d < np.sqrt(area_median)],\n",
    "    tree_i[tree_d < np.sqrt(area_median)]]).T\n",
    "pairs = np.vstack([pairs, pairs[:, ::-1]])\n",
    "pairs = np.unique(pairs, axis=0)\n",
    "pairs = pairs[pairs[:, 0] < pairs[:, 1], :]  # drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df.loc[[i for i, _ in pairs], :]\n",
    "df_j = df.loc[[j for _, j in pairs], :]\n",
    "df_i.loc[:, 'diff'] = (df_i.loc[:, 'cen_asset'].values -\n",
    "                       df_j.loc[:, 'cen_asset'].values)\n",
    "df_j.loc[:, 'diff'] = (df_j.loc[:, 'cen_asset'].values -\n",
    "                       df_i.loc[:, 'cen_asset'].values)\n",
    "\n",
    "df_i = df_i.reset_index(drop=True)\n",
    "df_i.loc[:, 'pair_id'] = df_i.index\n",
    "df_j = df_j.reset_index(drop=True)\n",
    "df_j.loc[:, 'pair_id'] = df_j.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_sample = (\n",
    "    df_i.loc[np.abs(df_i['diff']) > 0.5, 'pair_id'].tolist() +\n",
    "    df_i.loc[np.abs(df_i['diff']) <= 0.5, 'pair_id'].sample(n=50, random_state=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_i, df_j])\n",
    "df = df.loc[df['pair_id'].isin(pair_sample), :]\n",
    "# Int32 cannot be serialized, there is no N/A so safe to cast to int\n",
    "cols = df.select_dtypes(include='Int32').columns\n",
    "df.loc[:, cols] = df.loc[:, cols].astype('int')\n",
    "# save locality level census data\n",
    "df.to_file(OUT_LOC_DIR, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save chip level data\n",
    "df_chip = df.drop_duplicates(['ent', 'mun', 'loc']).loc[:, ['ent', 'mun', 'loc', 'geometry']]\n",
    "df_chip = aoi_to_chip(df=df_chip, indices=['ent', 'mun', 'loc'],\n",
    "                      file_name='ENT{:02d}MUN{:03d}LOC{:04d}CHIP{:06d}',\n",
    "                      input_type='polygon')\n",
    "df_chip.to_csv(OUT_IMG_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
